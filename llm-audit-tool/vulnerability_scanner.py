import os
import json
from pathlib import Path
from typing import List, Dict, Tuple
from llm_service import get_json_llm_response
import argparse
import re
import openai

class VulnerabilityScanner:
    def __init__(self, source_dir: str, chunk_size: int = 40, provider: str = "openai", model: str = None):
        self.source_dir = Path(source_dir)
        self.chunk_size = chunk_size
        self.supported_extensions = {'.go', '.cpp', '.py'}
        self.vulnerabilities = []
        self.provider = provider
        self.model = model
    
    def scan_directory(self) -> List[Dict[str, any]]:
        """Recursively scans the source directory and analyzes all supported files."""
        print(f"Starting scan in: {self.source_dir}")
        
        for file_path in self.source_dir.rglob('*'):
            if file_path.is_file() and file_path.suffix in self.supported_extensions:
                relative_path = file_path.relative_to(self.source_dir)
                print(f"\nAnalyzing file: {relative_path}")
                self._analyze_file(file_path, str(relative_path))
        
        return self.vulnerabilities
    
    def _analyze_file(self, file_path: Path, relative_path: str):
        """Analyzes a file by splitting it into chunks."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # Découper en chunks
            for chunk_start in range(0, len(lines), self.chunk_size):
                chunk_end = min(chunk_start + self.chunk_size, len(lines))
                chunk_lines = lines[chunk_start:chunk_end]
                
                # Préparer le code avec numérotation
                numbered_code = self._prepare_numbered_code(chunk_lines, chunk_start)
                
                # Analyser le chunk
                print(f"  Analyzing lines {chunk_start} to {chunk_end-1}...")
                chunk_vulns = self._analyze_chunk(numbered_code, relative_path, file_path.suffix)
                
                # Ajouter les vulnérabilités trouvées
                self.vulnerabilities.extend(chunk_vulns)
                
        except Exception as e:
            print(f"  Error analyzing {relative_path}: {e}")
     
    def _prepare_numbered_code(self, lines: List[str], start_line: int) -> str:
        """Prepares code with line numbering."""
        numbered_lines = []
        for i, line in enumerate(lines):
            line_number = start_line + i + 1 # To start from line 1
            numbered_lines.append(f"{line_number}: {line.rstrip()}")
        return '\n'.join(numbered_lines)
    
    def _analyze_chunk(self, numbered_code: str, file_path: str, file_extension: str) -> List[Dict[str, any]]:
        """Analyzes a chunk of code with the LLM."""
        language = {
            '.py': 'Python',
            '.go': 'Go',
            '.cpp': 'C++',
            '.hpp': 'C++'
        }.get(file_extension, 'Unknown')
        
        system_prompt = f"""
You are a senior application security researcher with deep expertise in vulnerability discovery, code review, and real-world exploitation. You are tasked with performing a thorough security audit of {language} code. Your analysis should:
- Identify vulnerabilities according to CWE, OWASP Top 10, and SANS Top 25.
- Consider a wide range of vulnerability types, including but not limited to: injection (SQL, command, code, etc.), authentication and authorization flaws, insecure design, cryptographic issues, logic errors, insecure dependencies, information disclosure, insecure file handling, memory safety, and improper error handling.
- Analyze the code contextually: consider data flow, trust boundaries, user input, privilege levels, error handling, and third-party/library usage.
- Only report real, exploitable vulnerabilities. Ignore theoretical or non-exploitable issues and false positives.
- For each finding, provide:
    - The exact line number (matching the code provided)
    - The CWE identifier (e.g., CWE-89)
    - A short justification (why it is a vulnerability, how it could be exploited, and a real-world scenario if possible)
    - A severity rating (High, Medium, Low) based on exploitability and impact
- Output ONLY a JSON array in the following format:
[
  {{"line": <line_number>, "cwe": "<CWE_identifier>", "severity": "<High|Medium|Low>", "justification": "<short explanation>"}}
]
If no vulnerabilities are found, return an empty array [].
"""
        user_prompt = f"""
Perform a comprehensive security review of the following {language} code. Use your expertise as a security researcher to:
- Check for all relevant vulnerability classes (CWE, OWASP Top 10, SANS Top 25, etc.)
- Consider injection, logic flaws, cryptography, insecure design, authentication/authorization, information disclosure, insecure dependencies, and other common and advanced bug classes.
- Be context-aware: consider how the code handles user input, authentication, authorization, error handling, and external calls.
- Only report real, exploitable vulnerabilities. Do NOT report theoretical or non-exploitable issues.
- For each finding, provide the line number, CWE, severity, and a short justification.
- Output ONLY a JSON array in the following format:
[
  {{"line": <line_number>, "cwe": "<CWE_identifier>", "severity": "<High|Medium|Low>", "justification": "<short explanation>"}}
]

Here is the code to analyze:

```{language.lower()}
{numbered_code}
```

Return ONLY the JSON array as specified above.
"""

        try:
            response = get_json_llm_response(user_prompt, system_prompt, self.provider, self.model)
            chunk_vulns = []
            if isinstance(response, list):
                for vuln in response:
                    if isinstance(vuln, dict) and 'line' in vuln and 'cwe' in vuln:
                        chunk_vulns.append({
                            "file": file_path,
                            "line": vuln['line'],
                            "cwe": vuln['cwe']
                        })
            return chunk_vulns
        except openai.RateLimitError as e:
            print("    Rate limit exceeded (429). Waiting 10 seconds before retrying...")
            import time
            time.sleep(10)
            try:
                response = get_json_llm_response(user_prompt, system_prompt, self.provider, self.model)
                chunk_vulns = []
                if isinstance(response, list):
                    for vuln in response:
                        if isinstance(vuln, dict) and 'line' in vuln and 'cwe' in vuln:
                            chunk_vulns.append({
                                "file": file_path,
                                "line": vuln['line'],
                                "cwe": vuln['cwe']
                            })
                return chunk_vulns
            except Exception as e:
                print(f"    Error during LLM analysis after retry: {e}")
                return []
        except Exception as e:
            print(f"    Error during LLM analysis: {e}")
            return []
    
    def save_results(self, output_file: str = "result.json"):
        """Saves the results in a JSON file."""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.vulnerabilities, f, indent=2)
        print(f"\nResults saved in: {output_file}")
    
    def compare_with_reference(self, reference_file: str) -> Dict[str, any]:
        """Compares the results with the reference file."""
        try:
            with open(reference_file, 'r', encoding='utf-8') as f:
                reference_vulns = json.load(f)
        except FileNotFoundError:
            print(f"Reference file {reference_file} not found")
            return {}
        
        # Convert to sets for easier comparison
        found_set = {(v['file'], v['line'], v['cwe']) for v in self.vulnerabilities}
        reference_set = {(v['file'], v['line'], v['cwe']) for v in reference_vulns}
        # Calculate metrics
        true_positives = found_set & reference_set
        false_positives = found_set - reference_set
        false_negatives = reference_set - found_set
        # Prepare the report
        final_score = len(true_positives) / (len(true_positives) + len(false_positives) + len(false_negatives)) if (len(true_positives) + len(false_positives) + len(false_negatives)) > 0 else 0
        report = {
            "total_reference": len(reference_vulns),
            "total_found": len(self.vulnerabilities),
            "true_positives": len(true_positives),
            "false_positives": len(false_positives),
            "false_negatives": len(false_negatives),
            "final_score": final_score,
            "true_positives_details": sorted(list(true_positives)),
            "false_positives_details": sorted(list(false_positives)),
            "false_negatives_details": sorted(list(false_negatives))
        }
        return report
    
    def print_comparison_report(self, report: Dict[str, any]):
        """Prints a formatted comparison report."""
        print("\n" + "="*60)
        print("COMPARISON REPORT")
        print("="*60)
        print(f"Reference vulnerabilities: {report['total_reference']}")
        print(f"Found vulnerabilities: {report['total_found']}")
        print(f"True positives: {report['true_positives']}")
        print(f"False positives: {report['false_positives']}")
        print(f"False negatives: {report['false_negatives']}")
        print(f"Final score (TP / (TP + FP + FN)): {report['final_score']:.2%}")
        if report['false_positives_details']:
            print("\n--- FALSE POSITIVES (found vulnerabilities not present in reference) ---")
            for file, line, cwe in report['false_positives_details']:
                print(f"  {file}:{line} - {cwe}")
        if report['false_negatives_details']:
            print("\n--- FALSE NEGATIVES (missed vulnerabilities) ---")
            for file, line, cwe in report['false_negatives_details']:
                print(f"  {file}:{line} - {cwe}")


# Exécution directe du scanner
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Vulnerability scanner with LLM.")
    parser.add_argument('--provider', choices=['openai', 'openrouter'], default='openai', help='LLM provider to use (default: openai)')
    parser.add_argument('--model', type=str, default=None, help='Model name to use (e.g., openai/gpt-4o-2024-05-13, openai/o4-mini-high)')
    args = parser.parse_args()
    # Fixed configuration
    source_directory = "../vuln-app/"
    reference_file = "../vuln-app/VULNERABILITIES.json"
    chunk_size = 40
    print("Vulnerability scanner")
    print(f"Source folder: {source_directory}")
    print(f"Reference file: {reference_file}")
    print(f"Chunk size: {chunk_size} lines")
    print(f"LLM provider: {args.provider}")
    if args.model:
        print(f"LLM model: {args.model}")
    print()
    # Check that the source directory exists
    if not os.path.exists(source_directory):
        print(f"Error: The folder '{source_directory}' does not exist.")
        exit(1)
    # Create and run the scanner
    scanner = VulnerabilityScanner(source_directory, chunk_size=chunk_size, provider=args.provider, model=args.model)
    # Scan all files
    vulnerabilities = scanner.scan_directory()
    # Triage vulnerabilities to eliminate false positives
    # scanner.triage_vulnerabilities_with_llm()
    # Prepare output folder and file
    # Folder is outside llm-audit-tool, named after the model (sanitize for folder name)
    model_name = args.model if args.model else ("openai-gpt-4o" if args.provider == "openai" else "deepseek-deepseek-r1-distill-llama-70b-free")
    safe_model_name = re.sub(r'[^a-zA-Z0-9_-]', '_', model_name)
    output_base = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', safe_model_name))
    os.makedirs(output_base, exist_ok=True)
    # Find next available result_comparison<ID>.json
    existing = [f for f in os.listdir(output_base) if re.match(r'result_comparison(\d+)\.json$', f)]
    ids = [int(re.match(r'result_comparison(\d+)\.json$', f).group(1)) for f in existing if re.match(r'result_comparison(\d+)\.json$', f)]
    next_id = max(ids) + 1 if ids else 1
    result_file = os.path.join(output_base, f'result_comparison{next_id}.json')
    # Save the results and report only in this file
    if os.path.exists(reference_file):
        report = scanner.compare_with_reference(reference_file)
        scanner.print_comparison_report(report)
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2)
        print(f"\nComparison report saved in: {result_file}")
    else:
        print(f"\nReference file {reference_file} not found. Comparison skipped.")
    print(f"\nScan complete. {len(vulnerabilities)} vulnerabilities found.")